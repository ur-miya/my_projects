{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b08848-04bf-45b6-8f00-617f2e913197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import re\n",
    "import heapq\n",
    "from collections import Counter\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445178a-8d71-4ee5-b191-53f2a5134cc1",
   "metadata": {},
   "source": [
    "Unfortunately, it was difficult for me to single out my favorite chapter, so I trusted a random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c516f3ef-526f-4da2-9e7d-c0800190ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hwlongfellow.org/poems_poem.php?pid=62'\n",
    "response = requests.get(url)\n",
    "soup_gen = BeautifulSoup(response.content, 'html.parser')\n",
    "pids = []\n",
    "\n",
    "for a in soup_gen.find_all('a', href=True):\n",
    "    href = a['href']\n",
    "    match = re.search(r'pid=(\\d+)', href)\n",
    "    if match:\n",
    "        pid = match.group(1)\n",
    "        pids.append(pid)\n",
    "pids = list(set(pids))\n",
    "if pids:\n",
    "    random_pid = random.choice(pids)\n",
    "    random_url = f\"https://www.hwlongfellow.org/poems_poem.php?pid={random_pid}\"\n",
    "\n",
    "response_new = requests.get(random_url)\n",
    "soup = BeautifulSoup(response_new.content, 'html.parser')\n",
    "\n",
    "element = soup.find('span', class_='page-subtitle')\n",
    "poem_text = None\n",
    "\n",
    "for poem_id in ['poem', 'poem-content', 'content', 'main']:\n",
    "    poem_text = soup.find('div', {'id': poem_id})\n",
    "    if poem_text:\n",
    "        break\n",
    "\n",
    "if poem_text:\n",
    "    full_text = poem_text.get_text()\n",
    "    lines = [line.strip() for line in full_text.split('\\n') if line.strip()]\n",
    "    filtered_lines = []\n",
    "    filtered_lines.append(element.get_text())\n",
    "    for line in lines:\n",
    "        if \"The Song of Hiawatha 1855\" in line:\n",
    "            break  \n",
    "        filtered_lines.append(line)\n",
    "    cleaned_text = '\\n'.join(filtered_lines)\n",
    "    #print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b23b00-6437-4a13-8c3b-ecd3bc3499d8",
   "metadata": {},
   "source": [
    "#### Here is the code realisation of Haffman algorithm\n",
    "Also there is the calculation of table size and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0706ff73-72aa-47db-befa-e5e0f8697877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 32224 bites\n",
      "Compressed size: 18762 bites\n",
      "Table size: 732 bites\n",
      "Compressed size with table size: 19494 bites\n",
      "Compression ratio: 1.7175\n",
      "Compression ratio with table: 1.6530\n",
      "Time of compressing: 0.002561 seconds\n"
     ]
    }
   ],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, char, freq):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "\n",
    "def build_huffman_tree(text):\n",
    "    # Building Haffman tree\n",
    "    frequency = Counter(text)\n",
    "    heap = [HuffmanNode(char, freq) for char, freq in frequency.items()]\n",
    "    heapq.heapify(heap)\n",
    "    \n",
    "    while len(heap) > 1:\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged = HuffmanNode(None, left.freq + right.freq)\n",
    "        merged.left = left\n",
    "        merged.right = right\n",
    "        heapq.heappush(heap, merged)\n",
    "    \n",
    "    return heap[0]\n",
    "\n",
    "def build_codes(node, current_code=\"\", codes=None):\n",
    "    # Building the code table\n",
    "    if codes is None:\n",
    "        codes = {}\n",
    "    \n",
    "    if node is None:\n",
    "        return\n",
    "    \n",
    "    if node.char is not None:\n",
    "        codes[node.char] = current_code\n",
    "        return codes\n",
    "    \n",
    "    build_codes(node.left, current_code + \"0\", codes)\n",
    "    build_codes(node.right, current_code + \"1\", codes)\n",
    "    \n",
    "    return codes\n",
    "\n",
    "def huffman_compress(text):\n",
    "    if not text:\n",
    "        return \"\", {}\n",
    "    \n",
    "    root = build_huffman_tree(text)\n",
    "    codes = build_codes(root)\n",
    "    compressed_bits = ''.join(codes[char] for char in text)\n",
    "    \n",
    "    return compressed_bits, codes\n",
    "\n",
    "def huffman_decompress(compressed_bits, codes):\n",
    "\n",
    "    reverse_codes = {v: k for k, v in codes.items()}\n",
    "    \n",
    "    current_code = \"\"\n",
    "    decompressed_text = \"\"\n",
    "    \n",
    "    for bit in compressed_bits:\n",
    "        current_code += bit\n",
    "        if current_code in reverse_codes:\n",
    "            decompressed_text += reverse_codes[current_code]\n",
    "            current_code = \"\"\n",
    "    \n",
    "    return decompressed_text\n",
    "\n",
    "def calculate_table_size(codes):\n",
    "    # Each character is stored as ASCII: 8 bits\n",
    "    # Each code length: log2(max_code_length) bits\n",
    "    if not codes:\n",
    "        return 0\n",
    "\n",
    "    max_code_length = max(len(code) for code in codes.values())\n",
    "    bits_for_length = math.ceil(math.log2(max_code_length + 1))\n",
    "    table_size_bits = 0\n",
    "    \n",
    "    for char, code in codes.items():\n",
    "        table_size_bits += 8\n",
    "        table_size_bits += bits_for_length\n",
    "   \n",
    "    return table_size_bits\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "compressed_bits, codes = huffman_compress(cleaned_text)\n",
    "all_time = time.perf_counter() - start_time\n",
    "original_size = len(cleaned_text) * 8\n",
    "compressed_size = len(compressed_bits)\n",
    "table_size = calculate_table_size(codes)\n",
    "total_compressed_size = compressed_size + table_size\n",
    "ratio = original_size / compressed_size if compressed_size > 0 else 0\n",
    "ratio_with_table = original_size / total_compressed_size if total_compressed_size > 0 else 0\n",
    "\n",
    "\n",
    "#print(\"Code table:\", codes)\n",
    "print(f\"Original size: {original_size} bites\")\n",
    "print(f\"Compressed size: {compressed_size} bites\" )\n",
    "print(f\"Table size: {table_size} bites\" )\n",
    "print(f\"Compressed size with table size: {total_compressed_size} bites\" )\n",
    "print(f\"Compression ratio: {ratio:.4f}\")\n",
    "print(f\"Compression ratio with table: {ratio_with_table:.4f}\")\n",
    "print(f\"Time of compressing: {all_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de29477-ff54-4134-aeff-790df1cd9316",
   "metadata": {},
   "source": [
    "Here is the process of decompressing, just for interest if two texts are matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b486bcb2-8e4a-4451-89b8-4e0e85460477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text matching: True\n",
      "Time of decompressing: 0.013420 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "decompressed_text = huffman_decompress(compressed_bits, codes)\n",
    "all_time = time.perf_counter() - start_time\n",
    "print(f\"Text matching: {cleaned_text == decompressed_text}\")\n",
    "print(f\"Time of decompressing: {all_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b7130d-af07-4cd7-a413-27ce4c99d56d",
   "metadata": {},
   "source": [
    "#### Here is the code realisation of Lempel-Ziv-Welch algorithm\n",
    "And a bit modified calculation of table size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0941b99-c1ea-4e89-ab27-250230eb05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lzw_compress(text) :\n",
    "    #Comressing via algorithm LZW\n",
    "    used_chars = set(text)\n",
    "    dictionary = {}\n",
    "    next_code = 0\n",
    "\n",
    "    for char in used_chars:\n",
    "        dictionary[char] = next_code\n",
    "        next_code += 1\n",
    "    \n",
    "    max_dict_size = 4096\n",
    "    result_codes = []\n",
    "    current_string = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        new_string = current_string + char\n",
    "        if new_string in dictionary:\n",
    "            current_string = new_string\n",
    "        else:\n",
    "            result_codes.append(dictionary[current_string])\n",
    "            if next_code < max_dict_size:\n",
    "                dictionary[new_string] = next_code\n",
    "                next_code += 1\n",
    "            \n",
    "            current_string = char\n",
    "\n",
    "    if current_string:\n",
    "        result_codes.append(dictionary[current_string])\n",
    "    \n",
    "    bit_sequence = codes_to_bits(result_codes, max_dict_size)\n",
    "    \n",
    "    return bit_sequence, dictionary\n",
    "\n",
    "def codes_to_bits(codes, max_dict_size = 4096):\n",
    "\n",
    "    if not codes:\n",
    "        return \"\"\n",
    "    \n",
    "    max_code = max(codes) if codes else 0\n",
    "    bits_per_code = math.ceil(math.log2(max_code + 1))\n",
    "\n",
    "    max_bits = math.ceil(math.log2(max_dict_size))\n",
    "    bits_per_code = min(bits_per_code, max_bits)\n",
    "\n",
    "    bit_string = \"\"\n",
    "    for code in codes:\n",
    "        bin_code = bin(code)[2:]\n",
    "        bin_code = bin_code.zfill(bits_per_code)\n",
    "        bit_string += bin_code\n",
    "    \n",
    "    return bit_string\n",
    "\n",
    "def lzw_decompress(bit_sequence, initial_dict):\n",
    "\n",
    "    codes = bits_to_codes(bit_sequence, initial_dict)\n",
    "    dictionary = {code: char for char, code in initial_dict.items()}\n",
    "    dict_size = len(dictionary)\n",
    "    max_dict_size = 4096\n",
    "    \n",
    "    result = []\n",
    "    previous_code = codes[0]\n",
    "    \n",
    "    if previous_code not in dictionary:\n",
    "        raise ValueError(f\"Invalid first code: {previous_code}\")\n",
    "    \n",
    "    result.append(dictionary[previous_code])\n",
    "    \n",
    "    for current_code in codes[1:]:\n",
    "        if current_code in dictionary:\n",
    "            current_string = dictionary[current_code]\n",
    "        elif current_code == dict_size:\n",
    "            current_string = dictionary[previous_code] + dictionary[previous_code][0]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid code: {current_code}\")\n",
    "        \n",
    "        result.append(current_string)\n",
    "        \n",
    "        if dict_size < max_dict_size:\n",
    "            new_string = dictionary[previous_code] + current_string[0]\n",
    "            dictionary[dict_size] = new_string\n",
    "            dict_size += 1\n",
    "        \n",
    "        previous_code = current_code\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "def bits_to_codes(bit_sequence, initial_dict):\n",
    "    if not bit_sequence:\n",
    "        return []\n",
    "\n",
    "    max_dict_size = len(initial_dict)\n",
    "    max_initial_code = max(initial_dict.values()) if initial_dict else 255\n",
    "\n",
    "    max_possible_code = max(max_initial_code, max_dict_size - 1)\n",
    "    bits_per_code = math.ceil(math.log2(max_possible_code + 1))\n",
    "    max_bits = math.ceil(math.log2(max_dict_size))\n",
    "    bits_per_code = min(bits_per_code, max_bits)\n",
    "\n",
    "    codes = []\n",
    "    for i in range(0, len(bit_sequence), bits_per_code):\n",
    "        code_bits = bit_sequence[i:i + bits_per_code]\n",
    "        if len(code_bits) < bits_per_code:\n",
    "            continue\n",
    "        code = int(code_bits, 2)\n",
    "        codes.append(code)\n",
    "    \n",
    "    return codes\n",
    "\n",
    "def calculate_table_size_lwz(codes):\n",
    "    if not codes:\n",
    "        return 0\n",
    "    max_string_length = max(len(string) for string in codes.keys())\n",
    "\n",
    "    bits_for_length = math.ceil(math.log2(max_string_length + 1)) if max_string_length > 0 else 1\n",
    "    \n",
    "    table_size_bits = 0\n",
    "    \n",
    "    for string, code in codes.items():\n",
    "        table_size_bits += 8 * len(string)\n",
    "        table_size_bits += bits_for_length\n",
    "        table_size_bits += 12\n",
    "    \n",
    "    return table_size_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "659fa354-9b35-4334-82b0-f50c9b266f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 32224 bites\n",
      "Compressed size: 19129 bites\n",
      "Table size: 75384 bites\n",
      "Compressed size with table size: 94513 bites\n",
      "Compression ratio: 1.6846\n",
      "Compression ratio with table: 0.3409\n",
      "Time of compressing: 0.008879 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "bit_sequence, result_codes = lzw_compress(cleaned_text)\n",
    "all_time = time.perf_counter() - start_time\n",
    "compressed_size_lwz = len(bit_sequence)\n",
    "table_size_lwz = calculate_table_size_lwz(result_codes)\n",
    "total_compressed_size_lwz = compressed_size_lwz + table_size_lwz\n",
    "ratio_lwz = original_size / compressed_size_lwz if compressed_size_lwz > 0 else 0\n",
    "ratio_with_table_lwz = original_size / total_compressed_size_lwz if total_compressed_size_lwz > 0 else 0\n",
    "\n",
    "print(f\"Original size: {original_size} bites\")\n",
    "print(f\"Compressed size: {compressed_size_lwz} bites\" )\n",
    "print(f\"Table size: {table_size_lwz} bites\" )\n",
    "print(f\"Compressed size with table size: {total_compressed_size_lwz} bites\" )\n",
    "print(f\"Compression ratio: {ratio_lwz:.4f}\")\n",
    "print(f\"Compression ratio with table: {ratio_with_table_lwz:.4f}\")\n",
    "print(f\"Time of compressing: {all_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595b189-c55d-4ddb-afc2-d711234d103f",
   "metadata": {},
   "source": [
    "Here is the process of decompressing, just for interest if two texts are matching\n",
    "It was more difficult than with Haffman algorithm because the decompessing algorithm (another version without transfering the code table) gave unknown binary symbols from encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2bb6a46-af87-4e1a-9060-23785d65638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text matching: True\n",
      "Time of decompressing: 0.006735 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "decompressed_text_lwz = lzw_decompress(bit_sequence, result_codes)\n",
    "all_time = time.perf_counter() - start_time\n",
    "print(f\"Text matching: {cleaned_text == decompressed_text_lwz}\")\n",
    "print(f\"Time of decompressing: {all_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08707a43-9e5e-4095-9618-bc1e4441e760",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. Huffman algorithm has better compression ratio (1.7175) than LWZ algorithm (1.6846), if we estimate without taking into account the size of the table.\n",
    "2. Huffman algorithm's compression ratio (1.6530) is also better with the addition of the table (LWZ: 0.3409). This is because the Haffman table stores only pairs symbol - code, and the LWZ table stores much more combinations of symbols and its codes are alse demand more memory space.\n",
    "3. The table size of Huffman algorithm is 732 bites.\n",
    "4. The tible size of LWZ algorithm is 75384 bits, that is almost a hundred times more than Huffman table.\n",
    "5. The processing time for Huffman algorithm compression is less than for LWZ one.\n",
    "6. In general for this dataset Huffman algorithm is more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289f572-e093-4d9a-ae73-c837ac486fd0",
   "metadata": {},
   "source": [
    "### The algorithm for embedding the name of a state in some worldwide dataset for some ML task\n",
    "\n",
    "##### Trigonometric encoding of geographical data of capitals.\n",
    "__Step 1.__ Getting data about the country: its capital and latitude, longitude, height above sea level\n",
    "\n",
    "__Step 2.__ Converting degrees to radians\n",
    "\n",
    "__Step 3.__ Using sines and cosines to normalize latitude and longitude degrees to avoid a strong gap for actually close coordinates (e.g. +179 degrees and -179 degrees)\n",
    "\n",
    "    For latitude (lat_sin, lat_cos), the sine changes from the South Pole to the North Pole, the cosine shows the distance from the poles\n",
    "    \n",
    "    For longitude (log_sin, log_cos), a pair of sine and cosine creates a cyclic representation\n",
    "    \n",
    "__Step 4.__ The height above sea level is standardized as a linear value\n",
    "\n",
    "__Step 5.__ Creating the final feature vector: __[ lat_sin, lat_cos, log_sin, log_cos, standardized height above sea level ]__\n",
    "\n",
    "The resulting vector for each capital describes its geographical location, taking into account the spherical closed shape of the Earth. This vector is the code of the corresponding capital."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
